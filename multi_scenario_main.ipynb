{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0a17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import requests\n",
    "import esda\n",
    "import libpysal\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "import pygeos\n",
    "import subprocess\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c55a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_raster(rasterfile_path, filename, target_path, rescale_factor):\n",
    "    # first determine pixel size for resampling\n",
    "    xres = 0\n",
    "    yres = 0\n",
    "    \n",
    "    out = subprocess.run([\"gdalinfo\",\"-json\",rasterfile_path],stdout=subprocess.PIPE)\n",
    "    raster_meta = json.loads(out.stdout.decode('utf-8'))\n",
    "    if 'geoTransform' in raster_meta:\n",
    "        xres = raster_meta['geoTransform'][1]\n",
    "        yres = raster_meta['geoTransform'][5]\n",
    "        xres = xres * rescale_factor\n",
    "        yres = yres * rescale_factor\n",
    "\n",
    "    if (xres != 0) and (yres != 0):\n",
    "        # resample raster\n",
    "        save_path = target_path +\"/\"+ filename + f\"_resample.tiff\"\n",
    "        subprocess.run([\"gdalwarp\",\"-r\",\"bilinear\",\"-of\",\"GTiff\",\"-tr\",str(xres),str(yres),rasterfile_path,save_path])\n",
    "\n",
    "        return save_path, raster_meta\n",
    "    \n",
    "\n",
    "def polygonize_fim(rasterfile_path):\n",
    "\n",
    "    # Extract target path and filename from the given raster file path\n",
    "    target_path = '/'.join(rasterfile_path.split('/')[:-1])\n",
    "    filename = rasterfile_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "\n",
    "    # Define paths\n",
    "    resample_path = target_path +\"/\"+ filename + f\"_resample.tiff\"\n",
    "    reclass_file = target_path + \"/\" + filename + \"_reclass.tiff\"\n",
    "    geojson_out = \"%s/%s.json\" % (target_path, filename)\n",
    "\n",
    "    for temp_path_ in [resample_path, reclass_file, geojson_out]:\n",
    "        if os.path.exists(temp_path_):\n",
    "            os.remove(temp_path_)\n",
    "\n",
    "    # Resample raster file to 10-times smaller\n",
    "    resample_path, raster_meta = resample_raster(rasterfile_path, filename, target_path, rescale_factor=4)\n",
    "\n",
    "    # Reclassify raster\n",
    "    '''\n",
    "    water_lvl = [0, 2, 6, 15, np.inf]  # Original inundation map value (underwater in feet)\n",
    "    water_lvl_recls = [-9999, 1, 2, 3, 4]\n",
    "    '''\n",
    "    outfile = \"--outfile=\"+reclass_file\n",
    "    no_data_val = raster_meta['bands'][0]['noDataValue']\n",
    "    # subprocess.run([\"gdal_calc.py\",\"-A\",resample_path,outfile,\"--calc=-9999*(A<=0)+1*((A>0)*(A<=2))+2*((A>2)*(A<=6))+3*((A>6)*(A<=15))+4*(A>15)\",\"--NoDataValue=-9999\"],stdout=subprocess.PIPE)\n",
    "    subprocess.run([\"gdal_calc.py\",\"-A\",resample_path,outfile,f\"--calc=-9999*(A<=0)+1*(A>0)\",f\"--NoDataValue={no_data_val}\"],stdout=subprocess.PIPE)\n",
    "        \n",
    "    # Polygonize the reclassified raster\n",
    "    subprocess.run([\"gdal_polygonize.py\", reclass_file, \"-b\", \"1\", geojson_out, filename, \"value\"])\n",
    "\n",
    "    inund_polygons = gpd.read_file(geojson_out)\n",
    "\n",
    "    if inund_polygons.shape[0] != 0:\n",
    "        inund_polygons = inund_polygons.loc[(inund_polygons['value'] != -9999) & (inund_polygons['value'] != 0)]  # Remove pixels of null value\n",
    "\n",
    "        # drop invalid geometries\n",
    "        inund_polygons = inund_polygons.loc[inund_polygons['geometry'].is_valid, :]\n",
    "\n",
    "        # Coverage for each class of inundation map\n",
    "        inund_per_cls = inund_polygons.dissolve(by='value')\n",
    "        inund_per_cls.reset_index(inplace=True)\n",
    "\n",
    "        # remove all temp files\n",
    "        os.remove(resample_path)\n",
    "        os.remove(reclass_file)\n",
    "        os.remove(geojson_out)\n",
    "\n",
    "        # inundation_per_cls: GeoDataFrame \n",
    "        return inund_per_cls\n",
    "\n",
    "    else:\n",
    "        return gpd.GeoDataFrame(data={'value': 1}, index=[0], geometry=[None])\n",
    "\n",
    "    # inundation_per_cls: GeoDataFrame \n",
    "    return inund_per_cls\n",
    "\n",
    "\n",
    "def fim_and_ellipse(dam_id, input_dir):\n",
    "    \n",
    "    sce_mh = {'loadCondition': 'MH', 'breachCondition': 'F'}  # Maximun Height scenario\n",
    "    sce_tas = {'loadCondition': 'TAS', 'breachCondition': 'F'}  # Top of Active Storage scenario\n",
    "    sce_nh = {'loadCondition': 'NH', 'breachCondition': 'F'}  # Normal Height scenario\n",
    "\n",
    "    # Maximun Height scenario (weight: 1)\n",
    "    fim_path_mh = f\"{input_dir}/NID_FIM_{sce_mh['loadCondition']}_{sce_mh['breachCondition']}/{sce_mh['loadCondition']}_{sce_mh['breachCondition']}_{dam_id}.tiff\"\n",
    "    fim_gdf_mh = polygonize_fim(fim_path_mh)\n",
    "    fim_gdf_mh['value_mh'] = fim_gdf_mh['value'] * 1\n",
    "    fim_gdf_mh.drop(columns=['value'], inplace=True)\n",
    "\n",
    "    # Top of Active Storage scenario (weight: 2)\n",
    "    fim_path_tas = f\"{input_dir}/NID_FIM_{sce_tas['loadCondition']}_{sce_tas['breachCondition']}/{sce_tas['loadCondition']}_{sce_tas['breachCondition']}_{dam_id}.tiff\"\n",
    "    fim_gdf_tas = polygonize_fim(fim_path_tas)\n",
    "    fim_gdf_tas['value_tas'] = fim_gdf_tas['value'] * 1\n",
    "    fim_gdf_tas.drop(columns=['value'], inplace=True)\n",
    "\n",
    "    # Normal Height scenario (weight: 4)\n",
    "    fim_path_nh = f\"{input_dir}/NID_FIM_{sce_nh['loadCondition']}_{sce_nh['breachCondition']}/{sce_nh['loadCondition']}_{sce_nh['breachCondition']}_{dam_id}.tiff\"\n",
    "    fim_gdf_nh = polygonize_fim(fim_path_nh)\n",
    "    fim_gdf_nh['value_nh'] = fim_gdf_nh['value'] * 1\n",
    "    fim_gdf_nh.drop(columns=['value'], inplace=True)\n",
    "\n",
    "    # Find intersections of inundated area across multiple scenarios\n",
    "    temp_fim_gdf = gpd.overlay(fim_gdf_nh, fim_gdf_tas, how='union')\n",
    "    fim_gdf = gpd.overlay(temp_fim_gdf, fim_gdf_mh, how='union')\n",
    "    fim_gdf.fillna(0, inplace=True)\n",
    "\n",
    "    # Sum values (1: MH only, 2: TAS only, 3: MH + TAS, 4: NH only, 5: MH + NH, 6: TAS + NH, 7: MH + TAS + NH)\n",
    "    fim_gdf['value'] = fim_gdf.apply(lambda x:x['value_mh'] + x['value_tas'] + x['value_nh'], axis=1)\n",
    "    fim_gdf.drop(columns=['value_mh', 'value_tas', 'value_nh'], inplace=True)\n",
    "    fim_gdf['Dam_ID'] = dam_id\n",
    "        \n",
    "    return fim_gdf\n",
    "\n",
    "\n",
    "def state_num_related_to_fim(fim_gdf, tract_gdf):\n",
    "    \n",
    "    tract_geoms = pygeos.from_shapely(tract_gdf['geometry'].values)\n",
    "    tract_geoms_tree = pygeos.STRtree(tract_geoms, leafsize=50)\n",
    "\n",
    "    fim_geom_union = pygeos.from_shapely(fim_gdf['geometry'].unary_union)    \n",
    "    query_intersect = tract_geoms_tree.query(fim_geom_union, predicate='intersects')\n",
    "    tract_gdf = tract_gdf.loc[query_intersect]\n",
    "\n",
    "    tract_gdf['STATE'] = tract_gdf.apply(lambda x:x['GEOID'][0:2], axis=1)\n",
    "    unique_state = tract_gdf['STATE'].unique()\n",
    "    \n",
    "    # return type: list\n",
    "    return unique_state\n",
    "    \n",
    "\n",
    "def extract_fim_geoid(dam_id, input_dir, tract_gdf):\n",
    "    print(f'{dam_id}: Step 1, 1/4, Identifying associated regions')\n",
    "    fim_gdf = fim_and_ellipse(dam_id, input_dir)\n",
    "\n",
    "    print(f'{dam_id}: Step 1, 2/4, Search states associated')\n",
    "    fim_state = state_num_related_to_fim(fim_gdf, tract_gdf)\n",
    "    print(f'-- {dam_id} impacts {len(fim_state)} States, {fim_state}')\n",
    "\n",
    "    if len(fim_state) == 1: # If only one state is associated with the inundation mapping\n",
    "        census_gdf = gpd.read_file(f'{input_dir}/census_geometry/tl_2020_{fim_state[0]}_tabblock20.geojson')\n",
    "    elif len(fim_state) >= 2: # If multiple states are associated with the inundation mapping\n",
    "        census_gdf = pd.DataFrame()\n",
    "        for state_num in fim_state:\n",
    "            temp_gdf = gpd.read_file(f'{input_dir}/census_geometry/tl_2020_{state_num}_tabblock20.geojson')\n",
    "            census_gdf = pd.concat([temp_gdf, census_gdf]).reset_index(drop=True)\n",
    "            census_gdf = gpd.GeoDataFrame(census_gdf, geometry=census_gdf['geometry'], crs=\"EPSG:4326\")\n",
    "    else:\n",
    "        raise AttributeError('NO STATE is related to Inundation Mapping')\n",
    "\n",
    "    # Destination dataframe to save the results\n",
    "    print(f\"{dam_id}: Step 1, 3/4, Extracting GEOID of census blocks\")\n",
    "    fim_geoid_df = pd.DataFrame({'Dam_ID': pd.Series(dtype='str'),\n",
    "                                'GEOID': pd.Series(dtype='str'),\n",
    "                                'Class': pd.Series(dtype='str')}\n",
    "                                )    \n",
    "\n",
    "    # Create STRtree for census_gdf\n",
    "    census_geoms = pygeos.from_shapely(census_gdf['geometry'].values)\n",
    "    census_geoms_tree = pygeos.STRtree(census_geoms, leafsize=50)\n",
    "\n",
    "    # Extract census tract intersecting with each class of inundation map\n",
    "    for water_cls in fim_gdf['value'].unique():\n",
    "        fim_geom_ = pygeos.from_shapely(fim_gdf.loc[fim_gdf['value'] == water_cls, 'geometry'].values[0])\n",
    "        query_fim_geom_ = census_geoms_tree.query(fim_geom_, predicate='intersects')\n",
    "        fim_geoid_ = census_gdf.loc[query_fim_geom_]\n",
    "\n",
    "        for geoid_ in fim_geoid_['GEOID'].to_list():\n",
    "            new_row = pd.DataFrame({'Dam_ID': dam_id, \n",
    "                                    'GEOID': geoid_, \n",
    "                                    'Class': water_cls}, \n",
    "                                    index=[0]\n",
    "                                    )\n",
    "            fim_geoid_df = pd.concat([new_row, fim_geoid_df]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"{dam_id}: Step 1, 4/4, Assigning geometry to census blocks\")\n",
    "    fim_geoid_gdf = fim_geoid_df.merge(census_gdf, on='GEOID')\n",
    "    fim_geoid_gdf = gpd.GeoDataFrame(fim_geoid_gdf, geometry=fim_geoid_gdf['geometry'], crs='EPSG:4326')\n",
    "    fim_geoid_gdf['Class'] = fim_geoid_gdf['Class'].astype(int)\n",
    "    fim_geoid_gdf = fim_geoid_gdf.groupby(['Dam_ID', 'GEOID'], \n",
    "                                    group_keys=False).apply(lambda x:x.loc[x['Class'].idxmax()]\n",
    "                                                            ).reset_index(drop=True)\n",
    "    fim_geoid_gdf = fim_geoid_gdf.set_crs(epsg=4326)\n",
    "\n",
    "    return fim_geoid_gdf, fim_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d34298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dams: 345\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.getcwd()\n",
    "\n",
    "sce_mh = {'loadCondition': 'MH', 'breachCondition': 'F'}  # Maximun Height scenario\n",
    "sce_tas = {'loadCondition': 'TAS', 'breachCondition': 'F'}  # Top of Active Storage scenario\n",
    "sce_nh = {'loadCondition': 'NH', 'breachCondition': 'F'}  # Normal Height scenario\n",
    "\n",
    "# Find the list of dams in the input folder\n",
    "fed_dams = pd.read_csv('./nid_available_scenario.csv')\n",
    "\n",
    "# Remove dams with error (fim is too small to generate)\n",
    "fed_dams = fed_dams.loc[fed_dams['ID'] != 'CO01283S001']\n",
    "\n",
    "# Select only Fed Dams that have inundation maps.\n",
    "for sce in [sce_mh, sce_tas, sce_nh]:\n",
    "    fed_dams = fed_dams.loc[fed_dams[f'{sce[\"loadCondition\"]}_{sce[\"breachCondition\"]}'] == True]\n",
    "    fed_dams = fed_dams.loc[fed_dams.apply(lambda x: True \n",
    "                                        if os.path.exists(os.path.join(data_dir, \n",
    "                                        f'NID_FIM_{sce[\"loadCondition\"]}_{sce[\"breachCondition\"]}', \n",
    "                                        f'{sce[\"loadCondition\"]}_{sce[\"breachCondition\"]}_{x[\"ID\"]}.tiff')\n",
    "                                        ) else False, axis=1)].reset_index(drop=True)\n",
    "fed_dams = gpd.GeoDataFrame(fed_dams, geometry=gpd.points_from_xy(fed_dams['LON'], fed_dams['LAT'], crs=\"EPSG:4326\"))\n",
    "print(f'Total Dams: {fed_dams.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb8dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census tract to find state associated with fim of each dam\n",
    "tract = gpd.read_file(os.path.join(data_dir, 'census_geometry', 'census_tract_from_api.geojson'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dbe4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_id = 'MS01496'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aad087c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS01496: Step 1, 1/4, Identifying associated regions\n",
      "Creating output file that is 93P x 235L.\n",
      "Processing /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_MH_F/MH_F_MS01496.tiff [1/1] : 0Using internal nodata values (e.g. 0) for image /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_MH_F/MH_F_MS01496.tiff.\n",
      "Copying nodata values from source /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_MH_F/MH_F_MS01496.tiff to destination /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_MH_F/MH_F_MS01496_resample.tiff.\n",
      "...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...Several drivers matching json extension. Using GeoJSON\n",
      "Creating output /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_MH_F/MH_F_MS01496.json of format GeoJSON.\n",
      "100 - done.\n",
      "Creating output file that is 181P x 467L.\n",
      "Processing /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_TAS_F/TAS_F_MS01496.tiff [1/1] : 0Using internal nodata values (e.g. -9999) for image /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_TAS_F/TAS_F_MS01496.tiff.\n",
      "Copying nodata values from source /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_TAS_F/TAS_F_MS01496.tiff to destination /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_TAS_F/TAS_F_MS01496_resample.tiff.\n",
      "...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...Several drivers matching json extension. Using GeoJSON\n",
      "Creating output /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_TAS_F/TAS_F_MS01496.json of format GeoJSON.\n",
      "100 - done.\n",
      "Creating output file that is 179P x 465L.\n",
      "Processing /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_NH_F/NH_F_MS01496.tiff [1/1] : 0Using internal nodata values (e.g. -9999) for image /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_NH_F/NH_F_MS01496.tiff.\n",
      "Copying nodata values from source /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_NH_F/NH_F_MS01496.tiff to destination /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_NH_F/NH_F_MS01496_resample.tiff.\n",
      "...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...Several drivers matching json extension. Using GeoJSON\n",
      "Creating output /Users/jparkgeo/Git_Repo/population_vulnerable_to_dam_failure/NID_FIM_NH_F/NH_F_MS01496.json of format GeoJSON.\n",
      "100 - done.\n",
      "MS01496: Step 1, 2/4, Search states associated\n",
      "-- MS01496 impacts 2 States, ['28' '22']\n",
      "MS01496: Step 1, 3/4, Extracting GEOID of census blocks\n",
      "MS01496: Step 1, 4/4, Assigning geometry to census blocks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dam_ID</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Class</th>\n",
       "      <th>HOUSING20</th>\n",
       "      <th>POP20</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>220659601001016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-91.09895 32.45693, -91.09342 32.457...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>220659601001018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-90.94706 32.33481, -90.94421 32.334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>220659601001288</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-90.90803 32.34336, -90.90697 32.344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>280159501002011</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((-90.10821 33.65497, -90.10791 33.655...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>280159501002013</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>POLYGON ((-90.10913 33.63145, -90.10897 33.632...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>281499503001046</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>POLYGON ((-90.87326 32.35128, -90.87314 32.351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>281499503002003</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((-90.87078 32.35200, -90.87067 32.352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>281499503002004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-90.87191 32.35213, -90.87189 32.352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>281499503002005</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>POLYGON ((-90.87209 32.35115, -90.87204 32.351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>MS01496</td>\n",
       "      <td>281499503002006</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>POLYGON ((-90.87095 32.35101, -90.87085 32.351...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2445 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dam_ID            GEOID  Class  HOUSING20  POP20  \\\n",
       "0     MS01496  220659601001016      1          0      0   \n",
       "1     MS01496  220659601001018      1          0      0   \n",
       "2     MS01496  220659601001288      3          0      0   \n",
       "3     MS01496  280159501002011      3          1      5   \n",
       "4     MS01496  280159501002013      3          6     16   \n",
       "...       ...              ...    ...        ...    ...   \n",
       "2440  MS01496  281499503001046      1         12     18   \n",
       "2441  MS01496  281499503002003      1          8      8   \n",
       "2442  MS01496  281499503002004      1          1      0   \n",
       "2443  MS01496  281499503002005      1         11     13   \n",
       "2444  MS01496  281499503002006      1          7     13   \n",
       "\n",
       "                                               geometry  \n",
       "0     POLYGON ((-91.09895 32.45693, -91.09342 32.457...  \n",
       "1     POLYGON ((-90.94706 32.33481, -90.94421 32.334...  \n",
       "2     POLYGON ((-90.90803 32.34336, -90.90697 32.344...  \n",
       "3     POLYGON ((-90.10821 33.65497, -90.10791 33.655...  \n",
       "4     POLYGON ((-90.10913 33.63145, -90.10897 33.632...  \n",
       "...                                                 ...  \n",
       "2440  POLYGON ((-90.87326 32.35128, -90.87314 32.351...  \n",
       "2441  POLYGON ((-90.87078 32.35200, -90.87067 32.352...  \n",
       "2442  POLYGON ((-90.87191 32.35213, -90.87189 32.352...  \n",
       "2443  POLYGON ((-90.87209 32.35115, -90.87204 32.351...  \n",
       "2444  POLYGON ((-90.87095 32.35101, -90.87085 32.351...  \n",
       "\n",
       "[2445 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_dic = {\n",
    "                \"EP_POV150\" : [['S1701_C01_040E'], 'S1701_C01_001E'],\n",
    "                \"EP_UNEMP\"  : 'DP03_0009PE',\n",
    "                \"EP_HBURD\"  : [['S2503_C01_028E', 'S2503_C01_032E', 'S2503_C01_036E', 'S2503_C01_040E'], \n",
    "                            'S2503_C01_001E'],\n",
    "                \"EP_NOHSDP\" : 'S0601_C01_033E',\n",
    "                \"EP_UNINSUR\" : 'S2701_C05_001E',\n",
    "                \"EP_AGE65\" : 'S0101_C02_030E',\n",
    "                \"EP_AGE17\" : [['B09001_001E'], \n",
    "                            'S0601_C01_001E'],\n",
    "                \"EP_DISABL\" : 'DP02_0072PE',\n",
    "                \"EP_SNGPNT\" : [['B11012_010E', 'B11012_015E'], 'DP02_0001E'],\n",
    "                \"EP_LIMENG\" : [['B16005_007E', 'B16005_008E', 'B16005_012E', 'B16005_013E', 'B16005_017E', 'B16005_018E', \n",
    "                                'B16005_022E', 'B16005_023E', 'B16005_029E', 'B16005_030E', 'B16005_034E', 'B16005_035E',\n",
    "                                'B16005_039E', 'B16005_040E', 'B16005_044E', 'B16005_045E'], \n",
    "                            'B16005_001E'],\n",
    "                \"EP_MINRTY\" : [['DP05_0071E', 'DP05_0078E', 'DP05_0079E', 'DP05_0080E', \n",
    "                                'DP05_0081E', 'DP05_0082E', 'DP05_0083E'],\n",
    "                            'S0601_C01_001E'],\n",
    "                \"EP_MUNIT\" : [['DP04_0012E', 'DP04_0013E'], \n",
    "                            'DP04_0001E'],\n",
    "                \"EP_MOBILE\" : 'DP04_0014PE',\n",
    "                \"EP_CROWD\" : [['DP04_0078E', 'DP04_0079E'], \n",
    "                            'DP04_0002E'],\n",
    "                \"EP_NOVEH\" : 'DP04_0058PE',\n",
    "                \"EP_GROUPQ\": [['B26001_001E'], \n",
    "                            'S0601_C01_001E'],\n",
    "}\n",
    "\n",
    "\n",
    "fim_geoid_gdf, fim_gdf = extract_fim_geoid(dam_id, data_dir, tract)\n",
    "fim_geoid_gdf\n",
    "# mi_gdf, lm_gdf = spatial_correlation(dam_id, fed_dams, fim_geoid_gdf, census_dic, API_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88b11097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim_geoid_gdf['Class'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
